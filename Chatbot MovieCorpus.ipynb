{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7924ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import pandas as pd\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a592be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip \"./datasets/cornell_movie_dialogs_corpus.zip\"\n",
    "# !unzip -q \"datasets/glove.6B.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59801304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "corpus = os.path.join(\"./datasets\", corpus_name)\n",
    "\n",
    "def printlines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "\n",
    "        for line in lines[:n]:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "printlines(os.path.join(corpus, \"movie_lines.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9269fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits lines into dictionary of fields\n",
    "def loadLines(filename, fields):\n",
    "    lines = {}\n",
    "    with open(filename, 'r', encoding=\"iso-8859-1\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "\n",
    "            lineObj = {}\n",
    "\n",
    "            for i, field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "      \n",
    "            lines[lineObj[\"lineID\"]] = lineObj\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Group fields according to convertions or movie_conversations\n",
    "def loadConversations(filename, fields, lines):\n",
    "    conversations = []\n",
    "    with open(filename, 'r', encoding=\"iso-8859-1\") as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "\n",
    "            conversationObj = {}\n",
    "\n",
    "            for i, field in enumerate(fields):\n",
    "                conversationObj[field] = values[i]\n",
    "\n",
    "            lineIds = re.findall(\"L[0-9]+\", conversationObj[\"utteranceIDs\"])\n",
    "\n",
    "            conversationObj[\"lines\"] = []\n",
    "\n",
    "            for lineId in lineIds:\n",
    "                conversationObj[\"lines\"].append(lines[lineId])\n",
    "\n",
    "            conversations.append(conversationObj)\n",
    "\n",
    "    return conversations\n",
    "\n",
    "# Make a group pair of chats (ask-reply)\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "\n",
    "    for conversationObj in conversations:\n",
    "        for i in range(len(conversationObj[\"lines\"]) - 1):\n",
    "            input_line = conversationObj[\"lines\"][i][\"text\"].strip()\n",
    "            target_line = conversationObj[\"lines\"][i+1][\"text\"].strip()\n",
    "\n",
    "            qa_pairs.append([input_line, target_line])\n",
    "\n",
    "  \n",
    "    return qa_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3042cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing lines...\n",
      "\n",
      "Processing conversations\n",
      "\n",
      "Create new formatted file\n",
      "\n",
      "Sample lines from file:\n",
      "b'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.,\"Well, I thought we\\'d start with pronunciation, if that\\'s okay with you.\"\\n'\n",
      "b'\"Well, I thought we\\'d start with pronunciation, if that\\'s okay with you.\",Not the hacking and gagging and spitting part.  Please.\\n'\n",
      "b\"Not the hacking and gagging and spitting part.  Please.,Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?,Forget it.\\n\"\n",
      "b'\"No, no, it\\'s my fault -- we didn\\'t have a proper introduction ---\",Cameron.\\n'\n",
      "b'Cameron.,\"The thing is, Cameron -- I\\'m at the mercy of a particularly hideous breed of loser.  My sister.  I can\\'t date until she does.\"\\n'\n",
      "b'\"The thing is, Cameron -- I\\'m at the mercy of a particularly hideous breed of loser.  My sister.  I can\\'t date until she does.\",Seems like she could get a date easy enough...\\n'\n",
      "b'Why?,\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\"\\n'\n",
      "b'\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\",That\\'s a shame.\\n'\n",
      "b'\"Gosh, if only we could find Kat a boyfriend...\",Let me see what I can do.\\n'\n"
     ]
    }
   ],
   "source": [
    "# Our new file\n",
    "datafile = os.path.join(corpus, \"formatted_movie_lines.csv\")\n",
    "\n",
    "# Tab delimiter for our new file\n",
    "delimiter = \"\\t\"\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode-escape\"))\n",
    "\n",
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "MOVIE_CONVERSATION_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "# Load lines and conversations\n",
    "print(\"\\nProcessing lines...\")\n",
    "lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "print(\"\\nProcessing conversations\")\n",
    "conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"), MOVIE_CONVERSATION_FIELDS, lines)\n",
    "\n",
    "print(\"\\nCreate new formatted file\")\n",
    "pd.DataFrame(extractSentencePairs(conversations)).to_csv(datafile, header = False, index=False)\n",
    "\n",
    "print(\"\\nSample lines from file:\")\n",
    "printlines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8635998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_dataset = pd.read_csv(datafile, names=['q','a'])\n",
    "qa_dataset['a'] = \"[sos] \" + qa_dataset['a'] + \" [eos]\"\n",
    "\n",
    "qa_dataset = qa_dataset[(qa_dataset['q'].str.count(\" \") < 10) & (qa_dataset['a'].str.count(\" \") < 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a9b406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q    0.0\n",
       "a    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * qa_dataset.isnull().sum()/len(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65462fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 15000\n",
    "sequence_length = 15\n",
    "batch_size = 32\n",
    "\n",
    "vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "vectorization.adapt(qa_dataset['q'].to_list())\n",
    "vectorization.adapt(qa_dataset['a'].to_list())\n",
    "\n",
    "voc = vectorization.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8020949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def format_dataset(que, ans):\n",
    "    que = vectorization(que)[:, :-1]\n",
    "    ans = vectorization(ans)\n",
    "    return ({\"encoder_inputs\": que, \"decoder_inputs\": ans[:, :-1]}, ans[:, 1:])\n",
    "\n",
    "def make_dataset(dataset):\n",
    "    que, ans = dataset['q'].to_list(), dataset['a'].to_list()\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((que, ans))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    \n",
    "    return dataset.shuffle(2048).prefetch(16).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "45c7f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = make_dataset(qa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1520f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (32, 14)\n",
      "inputs[\"decoder_inputs\"].shape: (32, 14)\n",
      "targets.shape: (32, 14)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98d53c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = \"datasets/glove.6B.300d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc62a682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12243 words (2757 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = vocab_size\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62632f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67136cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=\"int32\")\n",
    "        attention_output = self.attention(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
    "        )\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super(PositionalEmbedding, self).__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim,\n",
    "            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "            trainable = True,\n",
    "            mask_zero = True\n",
    "        )\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(TransformerDecoder, self).__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
    "        )\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
    "            axis=0,\n",
    "        )\n",
    "        return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b55bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = embedding_dim\n",
    "latent_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bf36fadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positional_embedding_6 (Positio (None, None, 300)    4504500     encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_4 (Transfor (None, None, 300)    4119848     positional_embedding_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "model_9 (Functional)            (None, None, 15000)  16027448    decoder_inputs[0][0]             \n",
      "                                                                 transformer_encoder_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 24,651,796\n",
      "Trainable params: 24,651,796\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/40\n",
      "681/681 [==============================] - 1584s 2s/step - loss: 1.8391 - accuracy: 0.3778\n",
      "Epoch 2/40\n",
      "681/681 [==============================] - 1580s 2s/step - loss: 1.7861 - accuracy: 0.3856\n",
      "Epoch 3/40\n",
      "681/681 [==============================] - 1584s 2s/step - loss: 1.7454 - accuracy: 0.3905\n",
      "Epoch 4/40\n",
      "681/681 [==============================] - 1578s 2s/step - loss: 1.7238 - accuracy: 0.3935\n",
      "Epoch 5/40\n",
      "681/681 [==============================] - 1574s 2s/step - loss: 1.7077 - accuracy: 0.3959\n",
      "Epoch 6/40\n",
      "681/681 [==============================] - 1572s 2s/step - loss: 1.6943 - accuracy: 0.3979\n",
      "Epoch 7/40\n",
      "681/681 [==============================] - 1573s 2s/step - loss: 1.6825 - accuracy: 0.4003\n",
      "Epoch 8/40\n",
      "681/681 [==============================] - 1571s 2s/step - loss: 1.6716 - accuracy: 0.4020\n",
      "Epoch 9/40\n",
      "681/681 [==============================] - 1571s 2s/step - loss: 1.6611 - accuracy: 0.4037\n",
      "Epoch 10/40\n",
      "681/681 [==============================] - 1570s 2s/step - loss: 1.6511 - accuracy: 0.4054\n",
      "Epoch 11/40\n",
      "681/681 [==============================] - 1572s 2s/step - loss: 1.6415 - accuracy: 0.4066\n",
      "Epoch 12/40\n",
      "681/681 [==============================] - 1570s 2s/step - loss: 1.6343 - accuracy: 0.4080\n",
      "Epoch 13/40\n",
      "681/681 [==============================] - 1570s 2s/step - loss: 1.6278 - accuracy: 0.4096\n",
      "Epoch 14/40\n",
      "681/681 [==============================] - 1573s 2s/step - loss: 1.6225 - accuracy: 0.4110\n",
      "Epoch 15/40\n",
      "681/681 [==============================] - 1572s 2s/step - loss: 1.6172 - accuracy: 0.4128\n",
      "Epoch 16/40\n",
      "681/681 [==============================] - 1574s 2s/step - loss: 1.6138 - accuracy: 0.4138\n",
      "Epoch 17/40\n",
      "681/681 [==============================] - 1572s 2s/step - loss: 1.6106 - accuracy: 0.4153\n",
      "Epoch 18/40\n",
      "681/681 [==============================] - 1572s 2s/step - loss: 1.6092 - accuracy: 0.4169\n",
      "Epoch 19/40\n",
      "681/681 [==============================] - 1571s 2s/step - loss: 1.6090 - accuracy: 0.4180\n",
      "Epoch 20/40\n",
      "681/681 [==============================] - 1570s 2s/step - loss: 1.6107 - accuracy: 0.4196\n",
      "Epoch 21/40\n",
      "681/681 [==============================] - 1575s 2s/step - loss: 1.6129 - accuracy: 0.4209\n",
      "Epoch 22/40\n",
      "681/681 [==============================] - 1570s 2s/step - loss: 1.6132 - accuracy: 0.4223\n",
      "Epoch 23/40\n",
      "681/681 [==============================] - 1577s 2s/step - loss: 1.6149 - accuracy: 0.4237\n",
      "Epoch 24/40\n",
      "681/681 [==============================] - 1578s 2s/step - loss: 1.6192 - accuracy: 0.4250\n",
      "Epoch 25/40\n",
      "681/681 [==============================] - 1571s 2s/step - loss: 1.6249 - accuracy: 0.4261\n",
      "Epoch 26/40\n",
      "681/681 [==============================] - 1570s 2s/step - loss: 1.6264 - accuracy: 0.4276\n",
      "Epoch 27/40\n",
      "681/681 [==============================] - 1576s 2s/step - loss: 1.6265 - accuracy: 0.4288\n",
      "Epoch 28/40\n",
      "681/681 [==============================] - 1574s 2s/step - loss: 1.6262 - accuracy: 0.4298\n",
      "Epoch 29/40\n",
      "681/681 [==============================] - 1573s 2s/step - loss: 1.6252 - accuracy: 0.4313\n",
      "Epoch 30/40\n",
      "681/681 [==============================] - 1571s 2s/step - loss: 1.6251 - accuracy: 0.4319\n",
      "Epoch 31/40\n",
      "681/681 [==============================] - 1580s 2s/step - loss: 1.6237 - accuracy: 0.4334\n",
      "Epoch 32/40\n",
      "681/681 [==============================] - 1583s 2s/step - loss: 1.6227 - accuracy: 0.4342\n",
      "Epoch 33/40\n",
      "681/681 [==============================] - 1582s 2s/step - loss: 1.6210 - accuracy: 0.4354\n",
      "Epoch 34/40\n",
      "681/681 [==============================] - 1579s 2s/step - loss: 1.6190 - accuracy: 0.4366\n",
      "Epoch 35/40\n",
      "681/681 [==============================] - 1578s 2s/step - loss: 1.6163 - accuracy: 0.4380\n",
      "Epoch 36/40\n",
      "681/681 [==============================] - 1578s 2s/step - loss: 1.6136 - accuracy: 0.4389\n",
      "Epoch 37/40\n",
      "681/681 [==============================] - 1579s 2s/step - loss: 1.6104 - accuracy: 0.4404\n",
      "Epoch 38/40\n",
      "681/681 [==============================] - 1585s 2s/step - loss: 1.6079 - accuracy: 0.4412\n",
      "Epoch 39/40\n",
      "681/681 [==============================] - 1590s 2s/step - loss: 1.6062 - accuracy: 0.4422\n",
      "Epoch 40/40\n",
      "681/681 [==============================] - 1597s 2s/step - loss: 1.6027 - accuracy: 0.4436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_12_layer_call_and_return_conditional_losses, embedding_12_layer_call_fn, embedding_13_layer_call_and_return_conditional_losses, embedding_13_layer_call_fn, multi_head_attention_12_layer_call_and_return_conditional_losses while saving (showing 5 of 150). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/my_model/assets\n",
      "/home/lokendra/jupyter/environment/lib/python3.8/site-packages/keras/utils/generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    }
   ],
   "source": [
    "epochs = 40  # This should be at least 30 for convergence\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "transformer.fit(train_ds, epochs=epochs)\n",
    "\n",
    "# Save the entire model as a SavedModel.\n",
    "transformer.save('saved_model/my_model')\n",
    "transformer.save_weights('saved_model/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1301228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "what\n",
      "what you know me?\n",
      "i dont know\n"
     ]
    }
   ],
   "source": [
    "max_decoded_sentence_length = sequence_length - 1\n",
    "# transformer = transformer.load_weights(\"saved_model/weights.index\")\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = vectorization([input_sentence])[:, 1:]\n",
    "    decoded_sentence = \"sos\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = vectorization([decoded_sentence])[:, :-1]\n",
    "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
    "\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = list(word_index.keys())[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "    \n",
    "        if sampled_token == \"eos\":\n",
    "            break\n",
    "    decoded_sentence = decoded_sentence[4:][:-4]\n",
    "    return decoded_sentence\n",
    "\n",
    "for _ in range(2):\n",
    "    input_sentence = input()\n",
    "    answer = decode_sequence(input_sentence)\n",
    "    print(answer)\n",
    "    if input_sentence == 'byy':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f63516",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"PositionalEmbedding\" : PositionalEmbedding,\n",
    "    \"TransformerDecoder\" :TransformerDecoder,\n",
    "    \"TransformerEncoder\" : TransformerEncoder,\n",
    "}\n",
    "\n",
    "transformer = keras.models.model_from_json(transformer.to_json(), custom_objects = models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed9922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45242346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
